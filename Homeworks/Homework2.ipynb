{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"Homework2.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lilyr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# pip install nltk\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import otter\n",
    "\n",
    "grader = otter.Notebook()\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(review):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    review = review.lower()\n",
    "\n",
    "    \n",
    "    review = emoji_pattern.sub(r'', review)   \n",
    "    review=re.sub(\"isn't\",'is not',review)\n",
    "    review=re.sub(\"he's\",'he is',review)\n",
    "\n",
    "    #using the examples above, replace any contractions with their base words\n",
    "    review = re.sub(\"i'm\",\"i am\",review)\n",
    "    review = re.sub(\"you're\",\"you are\",review)\n",
    "    review = re.sub(\"she's\", \"she is\", review)\n",
    "    review = re.sub(\"it's\", \"it is\", review)\n",
    "    review = re.sub(\"we're\", \"we are\", review)\n",
    "    review = re.sub(\"they're\", \"they are\", review)\n",
    "    review = re.sub(\"can't\", \"cannot\", review)\n",
    "    review = re.sub(\"don't\", \"do not\", review)\n",
    "    review = re.sub(\"won't\", \"will not\", review)\n",
    "    review = re.sub(\"shouldn't\", \"should not\", review)\n",
    "    review = re.sub(\"wouldn't\", \"would not\", review)\n",
    "    review = re.sub(\"couldn't\", \"could not\", review)\n",
    "    review = re.sub(\"aren't\", \"are not\", review)\n",
    "    review = re.sub(\"haven't\", \"have not\", review)\n",
    "    review = re.sub(\"hasn't\", \"has not\", review)\n",
    "    review = re.sub(\"hadn't\", \"had not\", review)\n",
    "    review = re.sub(\"wasn't\", \"was not\", review)\n",
    "    review = re.sub(\"weren't\", \"were not\", review)\n",
    "    review = re.sub(\"'ll\", \" will\", review)\n",
    "    review = re.sub(\"'s\", \" is\", review)\n",
    "\n",
    "\n",
    "    review = re.sub('https?://\\S+|www\\.\\S+', '', review)\n",
    "\n",
    "    #using the example above, remove characters such as newlines, punctuation, numbers, and other non-alphabetical characters\n",
    "    review = re.sub('[^0-9a-zA-Z\\s]+', '', review)\n",
    "    review = re.sub('\\n', '', review)\n",
    "    \n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in and clean data\n",
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "cleaned_data = data['review'].apply(clean_data)\n",
    "\n",
    "cleaned_data = pd.DataFrame(cleaned_data)\n",
    "cleaned_data['sentiment'] = data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that is crazy you will regret watching i am shocked this was not good there is no plot i could not watch it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clean_data(\"That's crazy. You'll regret watching. I'm shocked. This wasn't good. There's no plot! I couldn't watch it.\"))\n",
    "clean_data(\"That's crazy. You'll regret watching. I'm shocked. This wasn't good. There's no plot! I couldn't watch it.\") == 'that is crazy you will regret watching i am shocked this was not good there is no plot i could not watch it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "# this will take a long time to run, likely at least 10 minutes.\n",
    "cleaned_data['no_sw'] = cleaned_data['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find value counts of all words in all data points\n",
    "counter = Counter()\n",
    "for text in cleaned_data[\"no_sw\"].values:\n",
    "    for word in text.split():\n",
    "        counter[word] += 1\n",
    "\n",
    "# creates a set of the 10 most common words amongst all reviews\n",
    "FREQWORDS = set([w for (w, wc) in counter.most_common(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY_cleaned_data = cleaned_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie', 'film', 'made', 'make', 'story', 'characters', 'time', 'great', 'movies', 'br'}\n",
      "                                                  review sentiment  \\\n",
      "0      one of the other reviewers has mentioned that ...  positive   \n",
      "1      a wonderful little production br br the filmin...  positive   \n",
      "2      i thought this was a wonderful way to spend ti...  positive   \n",
      "3      basically there is a family where a little boy...  negative   \n",
      "4      petter mattei is love in the time of money is ...  positive   \n",
      "...                                                  ...       ...   \n",
      "49995  i thought this movie did a down right good job...  positive   \n",
      "49996  bad plot bad dialogue bad acting idiotic direc...  negative   \n",
      "49997  i am a catholic taught in parochial elementary...  negative   \n",
      "49998  i am going to have to disagree with the previo...  negative   \n",
      "49999  no one expects the star trek movies to be high...  negative   \n",
      "\n",
      "                                                   no_sw  \n",
      "0      reviewers mentioned watching 1 oz episode hook...  \n",
      "1      wonderful production filming technique unassum...  \n",
      "2      wonderful spend hot summer weekend sitting air...  \n",
      "3      basically family boy jake thinks zombie closet...  \n",
      "4      petter mattei love money visually stunning wat...  \n",
      "...                                                  ...  \n",
      "49995  job creative original expecting lotta fun dvd ...  \n",
      "49996  plot dialogue acting idiotic directing annoyin...  \n",
      "49997  catholic taught parochial elementary schools n...  \n",
      "49998  disagree previous comment side maltin rate exc...  \n",
      "49999  expects star trek high art fans expect episode...  \n",
      "\n",
      "[50000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def remove_freqwords(text, freqwords):\n",
    "    '''Given a review that has been cleaned, returns the review without the most frequently appearing words'''\n",
    "    #complete this function which removes any of the most frequent words in a given review that do not add value\n",
    "    text = ' '.join([word for word in text.split() if word not in (freqwords)])\n",
    "\n",
    "\n",
    "# remove frequent words that do not add value to the model\n",
    "print(FREQWORDS)\n",
    "cleaned_data['no_sw'] = cleaned_data['no_sw'].apply(lambda x: ' '.join([word for word in x.split() if word not in (FREQWORDS)]))\n",
    "# COPY_cleaned_data[\"no_sw\"] = COPY_cleaned_data[\"no_sw\"].apply(lambda text: remove_freqwords(text, FREQWORDS))\n",
    "print(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sentiment                                             review\n",
      "0              1  [reviewers, mentioned, watching, 1, oz, episod...\n",
      "1              1  [wonderful, production, filming, technique, un...\n",
      "2              1  [wonderful, spend, hot, summer, weekend, sitti...\n",
      "3              0  [basically, family, boy, jake, thinks, zombie,...\n",
      "4              1  [petter, mattei, love, money, visually, stunni...\n",
      "...          ...                                                ...\n",
      "49995          1  [job, creative, original, expecting, lotta, fu...\n",
      "49996          0  [plot, dialogue, acting, idiotic, directing, a...\n",
      "49997          0  [catholic, taught, parochial, elementary, scho...\n",
      "49998          0  [disagree, previous, comment, side, maltin, ra...\n",
      "49999          0  [expects, star, trek, high, art, fans, expect,...\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# replace the sentiment with binary 0 or 1, prepare for model\n",
    "#tokenized_data = cleaned_data.drop(columns=['review'])\n",
    "tokenized_data = cleaned_data.drop(columns=['review'])\n",
    "tokenized_data.columns = ['sentiment', 'review']\n",
    "#print(tokenized_data)\n",
    "\n",
    "#TODO - convert the sentiment values to a binary 0 or 1\n",
    "tokenized_data['sentiment'] = tokenized_data['sentiment'].replace({'negative': 0, 'positive': 1})\n",
    "\n",
    "\n",
    "#TODO - complete data preprocessing by changing the review format from a string of words to a list of strings containing individual words\n",
    "tokenized_data['review'] = tokenized_data['review'].apply(lambda string: string.split())\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Part 2: Naive Bayes Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NaiveBayesModel:\n",
    "    '''Class representing the implementation of the Naive Bayes model'''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.occurrence_table = {}\n",
    "        self.probability_table = {}\n",
    "        self.labels = []\n",
    "\n",
    "    def train_model(self, data, labels):\n",
    "        '''Runs the training process for the model, building the occurrence table and probability table'''\n",
    "\n",
    "        #complete this function which runs the entire training process of Naive Bayes\n",
    "        NaiveBayesModel.build_occurrence_table(self, data, labels)\n",
    "        NaiveBayesModel.build_probability_table(self)\n",
    "        NaiveBayesModel.predict(self, data)\n",
    "\n",
    "\n",
    "    def build_occurrence_table(self, data, labels):\n",
    "        '''Private function to create the occurrence table given the training data and labels'''\n",
    "\n",
    "        #complete this function which creates a nested dictionary table of frequencies based on the training data.\n",
    "        # example format for the word 'word': {'word': {0: 3, 1: 15}}\n",
    "\n",
    "        for index,value in data.items():\n",
    "            for word in value:\n",
    "                if word not in self.occurrence_table.keys():\n",
    "                    self.occurrence_table[word]= {0:1, 1:1}\n",
    "                    # self.occurrence_table[word][labels[index]] += 1\n",
    "                else:\n",
    "                    # if labels[index] not in self.occurrence_table[word].keys():\n",
    "                        # self.occurrence_table[word][labels[index]] = 1\n",
    "                    # else:\n",
    "                    self.occurrence_table[word][labels[index]] += 1\n",
    "\n",
    "    def build_probability_table(self):\n",
    "        '''Private function to create the probability table based on the occurrence table'''\n",
    "\n",
    "        #complete this function which generates a table of probabilities based on the frequencies recorded in the occurrence table\n",
    "        # example format for the word 'word': {'word': {0: 0.1667, 1: 0.8333}}\n",
    "        for word in self.occurrence_table.keys():\n",
    "            self.probability_table[word] = {}\n",
    "            self.probability_table[word][0] = (self.occurrence_table[word][0]/(self.occurrence_table[word][0]+self.occurrence_table[word][1]))\n",
    "            # self.probability_table[word] = {}\n",
    "            self.probability_table[word][1] = (self.occurrence_table[word][1]/(self.occurrence_table[word][0]+self.occurrence_table[word][1]))\n",
    "\n",
    "\n",
    "    def predict(self, variables):\n",
    "        '''Takes a set of variables, and predicts the class they should belong to'''\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        #implement prediction using the Naive Bayes method\n",
    "        # remember that if the model has the same probability of either class, it should pick randomly between the two\n",
    "        for index,review in variables.items():\n",
    "            zero_prob = 1\n",
    "            one_prob = 1\n",
    "            for word in review:\n",
    "                if word in self.probability_table.keys():\n",
    "                    zero_prob *= self.probability_table[word][0]\n",
    "                    one_prob *= self.probability_table[word][1]\n",
    "            prediction = -1\n",
    "            if zero_prob > one_prob:\n",
    "                prediction = 0\n",
    "            elif (one_prob > zero_prob):\n",
    "                prediction = 1\n",
    "            else:\n",
    "                prediction = random.choice([zero_prob, one_prob])\n",
    "            predictions.append(prediction)\n",
    "\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# perform a train/test split on the data\n",
    "X=tokenized_data['review']\n",
    "y=tokenized_data['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=30)\n",
    "\n",
    "# train and predict using the Naive Bayes model implementation\n",
    "nb_model = NaiveBayesModel()\n",
    "nb_model.train_model(X_train, y_train)\n",
    "pred = nb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3944 1033]\n",
      " [ 412 4611]]\n",
      "0.8555\n",
      "0.9054178145087236\n",
      "0.8451730418943534\n"
     ]
    }
   ],
   "source": [
    "#Create and display confusion matrices and evaluation metrics to prove the performance of your model.\n",
    "# required: accuracy, recall, f-score\n",
    "\n",
    "# confusion matrix\n",
    "\n",
    "confusion_matrix = sklearn.metrics.confusion_matrix(y_test,pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "# accuracy\n",
    "print((pred==y_test).sum()/len(pred))\n",
    "# recall\n",
    "print((confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[1][0])))\n",
    "# f-score\n",
    "print((confusion_matrix[0][0]/(confusion_matrix[0][0] + 0.5 * (confusion_matrix[0][1] + confusion_matrix[1][0]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not resolve notebook path",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save your notebook first, then run this cell to export your submission.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgrader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_tests\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\otter\\check\\utils.py:184\u001b[0m, in \u001b[0;36mgrading_mode_disabled\u001b[1;34m(wrapped, self, args, kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_grading_mode:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\otter\\check\\utils.py:166\u001b[0m, in \u001b[0;36mincompatible_with.<locals>.incompatible\u001b[1;34m(wrapped, self, args, kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\otter\\check\\utils.py:217\u001b[0m, in \u001b[0;36mlogs_event.<locals>.event_logger\u001b[1;34m(wrapped, self, args, kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_event(event_type, success\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     ret \u001b[38;5;241m=\u001b[39m LoggedEventReturnValue(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\otter\\check\\utils.py:213\u001b[0m, in \u001b[0;36mlogs_event.<locals>.event_logger\u001b[1;34m(wrapped, self, args, kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03mRuns a method, catching any errors and logging the call. Returns the unwrapped return value\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03mof the wrapped function.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     ret: Optional[LoggedEventReturnValue[T]] \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_event(event_type, success\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, error\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\otter\\check\\notebook.py:427\u001b[0m, in \u001b[0;36mNotebook.export\u001b[1;34m(self, nb_path, export_path, pdf, filtering, pagebreaks, files, display_link, force_save, run_tests)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_event(EventType\u001b[38;5;241m.\u001b[39mBEGIN_EXPORT)\n\u001b[0;32m    425\u001b[0m files \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m files\n\u001b[1;32m--> 427\u001b[0m nb_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_nb_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResolved notebook path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_save:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\otter\\check\\notebook.py:236\u001b[0m, in \u001b[0;36mNotebook._resolve_nb_path\u001b[1;34m(self, nb_path, fail_silently)\u001b[0m\n\u001b[0;32m    233\u001b[0m         nb_path \u001b[38;5;241m=\u001b[39m notebooks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nb_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fail_silently:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not resolve notebook path\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nb_path\n",
      "\u001b[1;31mValueError\u001b[0m: Could not resolve notebook path"
     ]
    }
   ],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "naivebayes",
   "tests": {
    "p1": {
     "name": "p1",
     "points": null,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
